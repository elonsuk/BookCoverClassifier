{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0553e693",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9949f932",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = r\"C:\\Users\\offco\\Documents\\Dev_Projects\\BookCoverClassifier\\Datasets\\Test_Run\\Test_Augment\"\n",
    "\n",
    "mean = [0.3979, 0.3628, 0.3351]\n",
    "std = [0.3045, 0.2798, 0.2624]\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(torch.Tensor(mean), torch.Tensor(std)),\n",
    "])\n",
    "\n",
    "data = torchvision.datasets.ImageFolder(root=DATA_PATH, transform=transform)\n",
    "\n",
    "train_size = int(0.85 * len(data))\n",
    "test_size = len(data) - train_size\n",
    "train_data, test_data = torch.utils.data.random_split(data, [train_size, test_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9adb81ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(train_data, batch_size=32, shuffle=True, num_workers=4, pin_memory=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_data, batch_size=32, shuffle=False, num_workers=4, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5ac74c3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.models as models\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b0f823b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_device():\n",
    "    if torch.cuda.is_available(): dev = 'cuda:0'\n",
    "    else: dev = 'cpu'\n",
    "    \n",
    "    return torch.device(dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "78e4d534",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, test_loader):\n",
    "    model.eval()\n",
    "    \n",
    "    predicted_correctly_on_epoch = 0\n",
    "    total = 0\n",
    "    \n",
    "    device = set_device()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for data in test_loader:\n",
    "            images, labels = data\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            total += labels.size(0)\n",
    "            \n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            \n",
    "            predicted_correctly_on_epoch += (predicted == labels).sum().item()\n",
    "    \n",
    "    epoch_acc = 100.0 * (predicted_correctly_on_epoch / total)\n",
    "    print(f\"Testing dataset: {int(predicted_correctly_on_epoch)} out of {total} correct. Accuracy: {round(epoch_acc, 3)}\")\n",
    "    \n",
    "    return epoch_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f2e1e2c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "CHECKPOINT_PATH = r\"C:\\Users\\offco\\Documents\\Dev_Projects\\BookCoverClassifier\\Test_Run\\Model\\PyTorch Model\"\n",
    "\n",
    "def save_checkpoint(model, epoch, optimizer, best_acc):\n",
    "    state = {\n",
    "        'epoch': epoch+1,\n",
    "        'model': model.state_dict(),\n",
    "        'best accuracy': best_acc,\n",
    "        'optimizer': optimizer.state_dict()\n",
    "    }\n",
    "    \n",
    "    torch.save(state, f'{CHECKPOINT_PATH}\\model_best_checkpoint.pth.tar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "adc9ebb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_nn(model, train_loader, test_loader, criterion, optimizer, n_epochs):\n",
    "    device = set_device()\n",
    "    best_acc = 0\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        print(f\"Epoch: {epoch+1}\")\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        running_correct = 0.0\n",
    "        total = 0\n",
    "        \n",
    "        for data in train_loader:\n",
    "            images, labels = data\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            total += labels.size(0)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            \n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            \n",
    "            optimizer.step()\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "            running_correct += (labels == predicted).sum().item()\n",
    "        \n",
    "        epoch_loss = running_loss / len(train_loader)\n",
    "        epoch_acc = 100.00 * (running_correct / total)\n",
    "        \n",
    "        print(f\"Training dataset: {int(running_correct)} out of {total} correct. Accuracy: {round(epoch_acc, 3)}. Loss: {round(epoch_loss, 3)}\")\n",
    "        \n",
    "        test_acc = evaluate_model(model, test_loader)\n",
    "        \n",
    "        if (test_acc > best_acc):\n",
    "            best_acc = test_acc\n",
    "            save_checkpoint(model, epoch, optimizer, best_acc)\n",
    "        \n",
    "    print(\"Finished\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5bcd0555",
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet18 = models.resnet18(pretrained=False)\n",
    "num_features = resnet18.fc.in_features\n",
    "num_classes = 7\n",
    "resnet18.fc = nn.Linear(num_features, num_classes)\n",
    "\n",
    "device = set_device()\n",
    "resnet18 = resnet18.to(device)\n",
    "\n",
    "loss_func = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(resnet18.parameters(), lr=0.01, momentum=0.9, weight_decay=0.003)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f3849699",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n",
      "Training dataset: 25812 out of 35700 correct. Accuracy: 72.303. Loss: 0.792\n",
      "Testing dataset: 4116 out of 6300 correct. Accuracy: 65.333\n",
      "Epoch: 2\n",
      "Training dataset: 32944 out of 35700 correct. Accuracy: 92.28. Loss: 0.229\n",
      "Testing dataset: 5935 out of 6300 correct. Accuracy: 94.206\n",
      "Epoch: 3\n",
      "Training dataset: 34054 out of 35700 correct. Accuracy: 95.389. Loss: 0.138\n",
      "Testing dataset: 5970 out of 6300 correct. Accuracy: 94.762\n",
      "Epoch: 4\n",
      "Training dataset: 34308 out of 35700 correct. Accuracy: 96.101. Loss: 0.123\n",
      "Testing dataset: 5974 out of 6300 correct. Accuracy: 94.825\n",
      "Epoch: 5\n",
      "Training dataset: 34622 out of 35700 correct. Accuracy: 96.98. Loss: 0.096\n",
      "Testing dataset: 6142 out of 6300 correct. Accuracy: 97.492\n",
      "Epoch: 6\n",
      "Training dataset: 34463 out of 35700 correct. Accuracy: 96.535. Loss: 0.109\n",
      "Testing dataset: 6138 out of 6300 correct. Accuracy: 97.429\n",
      "Epoch: 7\n",
      "Training dataset: 34574 out of 35700 correct. Accuracy: 96.846. Loss: 0.102\n",
      "Testing dataset: 6153 out of 6300 correct. Accuracy: 97.667\n",
      "Epoch: 8\n",
      "Training dataset: 34553 out of 35700 correct. Accuracy: 96.787. Loss: 0.102\n",
      "Testing dataset: 6064 out of 6300 correct. Accuracy: 96.254\n",
      "Epoch: 9\n",
      "Training dataset: 34650 out of 35700 correct. Accuracy: 97.059. Loss: 0.093\n",
      "Testing dataset: 6213 out of 6300 correct. Accuracy: 98.619\n",
      "Epoch: 10\n",
      "Training dataset: 34767 out of 35700 correct. Accuracy: 97.387. Loss: 0.081\n",
      "Testing dataset: 6168 out of 6300 correct. Accuracy: 97.905\n",
      "Epoch: 11\n",
      "Training dataset: 34759 out of 35700 correct. Accuracy: 97.364. Loss: 0.081\n",
      "Testing dataset: 6169 out of 6300 correct. Accuracy: 97.921\n",
      "Epoch: 12\n",
      "Training dataset: 34704 out of 35700 correct. Accuracy: 97.21. Loss: 0.086\n",
      "Testing dataset: 5959 out of 6300 correct. Accuracy: 94.587\n",
      "Epoch: 13\n",
      "Training dataset: 34806 out of 35700 correct. Accuracy: 97.496. Loss: 0.083\n",
      "Testing dataset: 6210 out of 6300 correct. Accuracy: 98.571\n",
      "Epoch: 14\n",
      "Training dataset: 34701 out of 35700 correct. Accuracy: 97.202. Loss: 0.089\n",
      "Testing dataset: 6206 out of 6300 correct. Accuracy: 98.508\n",
      "Epoch: 15\n",
      "Training dataset: 34748 out of 35700 correct. Accuracy: 97.333. Loss: 0.082\n",
      "Testing dataset: 6210 out of 6300 correct. Accuracy: 98.571\n",
      "Epoch: 16\n",
      "Training dataset: 34894 out of 35700 correct. Accuracy: 97.742. Loss: 0.071\n",
      "Testing dataset: 6053 out of 6300 correct. Accuracy: 96.079\n",
      "Epoch: 17\n",
      "Training dataset: 34811 out of 35700 correct. Accuracy: 97.51. Loss: 0.077\n",
      "Testing dataset: 6136 out of 6300 correct. Accuracy: 97.397\n",
      "Epoch: 18\n",
      "Training dataset: 34935 out of 35700 correct. Accuracy: 97.857. Loss: 0.067\n",
      "Testing dataset: 6161 out of 6300 correct. Accuracy: 97.794\n",
      "Epoch: 19\n",
      "Training dataset: 34645 out of 35700 correct. Accuracy: 97.045. Loss: 0.089\n",
      "Testing dataset: 6008 out of 6300 correct. Accuracy: 95.365\n",
      "Epoch: 20\n",
      "Training dataset: 34941 out of 35700 correct. Accuracy: 97.874. Loss: 0.066\n",
      "Testing dataset: 6147 out of 6300 correct. Accuracy: 97.571\n",
      "Epoch: 21\n",
      "Training dataset: 34896 out of 35700 correct. Accuracy: 97.748. Loss: 0.067\n",
      "Testing dataset: 6216 out of 6300 correct. Accuracy: 98.667\n",
      "Epoch: 22\n",
      "Training dataset: 34699 out of 35700 correct. Accuracy: 97.196. Loss: 0.088\n",
      "Testing dataset: 6209 out of 6300 correct. Accuracy: 98.556\n",
      "Epoch: 23\n",
      "Training dataset: 34823 out of 35700 correct. Accuracy: 97.543. Loss: 0.075\n",
      "Testing dataset: 6017 out of 6300 correct. Accuracy: 95.508\n",
      "Epoch: 24\n",
      "Training dataset: 34865 out of 35700 correct. Accuracy: 97.661. Loss: 0.07\n",
      "Testing dataset: 6176 out of 6300 correct. Accuracy: 98.032\n",
      "Epoch: 25\n",
      "Training dataset: 35088 out of 35700 correct. Accuracy: 98.286. Loss: 0.049\n",
      "Testing dataset: 5075 out of 6300 correct. Accuracy: 80.556\n",
      "Epoch: 26\n",
      "Training dataset: 34711 out of 35700 correct. Accuracy: 97.23. Loss: 0.088\n",
      "Testing dataset: 6078 out of 6300 correct. Accuracy: 96.476\n",
      "Epoch: 27\n",
      "Training dataset: 34882 out of 35700 correct. Accuracy: 97.709. Loss: 0.073\n",
      "Testing dataset: 6099 out of 6300 correct. Accuracy: 96.81\n",
      "Epoch: 28\n",
      "Training dataset: 34844 out of 35700 correct. Accuracy: 97.602. Loss: 0.069\n",
      "Testing dataset: 5042 out of 6300 correct. Accuracy: 80.032\n",
      "Epoch: 29\n",
      "Training dataset: 34987 out of 35700 correct. Accuracy: 98.003. Loss: 0.061\n",
      "Testing dataset: 6211 out of 6300 correct. Accuracy: 98.587\n",
      "Epoch: 30\n",
      "Training dataset: 34729 out of 35700 correct. Accuracy: 97.28. Loss: 0.087\n",
      "Testing dataset: 6215 out of 6300 correct. Accuracy: 98.651\n",
      "Epoch: 31\n",
      "Training dataset: 35177 out of 35700 correct. Accuracy: 98.535. Loss: 0.041\n",
      "Testing dataset: 5175 out of 6300 correct. Accuracy: 82.143\n",
      "Epoch: 32\n",
      "Training dataset: 34692 out of 35700 correct. Accuracy: 97.176. Loss: 0.083\n",
      "Testing dataset: 6210 out of 6300 correct. Accuracy: 98.571\n",
      "Epoch: 33\n",
      "Training dataset: 34669 out of 35700 correct. Accuracy: 97.112. Loss: 0.091\n",
      "Testing dataset: 6156 out of 6300 correct. Accuracy: 97.714\n",
      "Epoch: 34\n",
      "Training dataset: 34884 out of 35700 correct. Accuracy: 97.714. Loss: 0.066\n",
      "Testing dataset: 5920 out of 6300 correct. Accuracy: 93.968\n",
      "Epoch: 35\n",
      "Training dataset: 34970 out of 35700 correct. Accuracy: 97.955. Loss: 0.059\n",
      "Testing dataset: 6215 out of 6300 correct. Accuracy: 98.651\n",
      "Epoch: 36\n",
      "Training dataset: 34838 out of 35700 correct. Accuracy: 97.585. Loss: 0.077\n",
      "Testing dataset: 6212 out of 6300 correct. Accuracy: 98.603\n",
      "Epoch: 37\n",
      "Training dataset: 34857 out of 35700 correct. Accuracy: 97.639. Loss: 0.072\n",
      "Testing dataset: 6139 out of 6300 correct. Accuracy: 97.444\n",
      "Epoch: 38\n",
      "Training dataset: 35022 out of 35700 correct. Accuracy: 98.101. Loss: 0.055\n",
      "Testing dataset: 6211 out of 6300 correct. Accuracy: 98.587\n",
      "Epoch: 39\n",
      "Training dataset: 34773 out of 35700 correct. Accuracy: 97.403. Loss: 0.086\n",
      "Testing dataset: 6201 out of 6300 correct. Accuracy: 98.429\n",
      "Epoch: 40\n",
      "Training dataset: 34962 out of 35700 correct. Accuracy: 97.933. Loss: 0.062\n",
      "Testing dataset: 6100 out of 6300 correct. Accuracy: 96.825\n",
      "Epoch: 41\n",
      "Training dataset: 34955 out of 35700 correct. Accuracy: 97.913. Loss: 0.063\n",
      "Testing dataset: 6207 out of 6300 correct. Accuracy: 98.524\n",
      "Epoch: 42\n",
      "Training dataset: 34844 out of 35700 correct. Accuracy: 97.602. Loss: 0.069\n",
      "Testing dataset: 5594 out of 6300 correct. Accuracy: 88.794\n",
      "Epoch: 43\n",
      "Training dataset: 34905 out of 35700 correct. Accuracy: 97.773. Loss: 0.066\n",
      "Testing dataset: 6200 out of 6300 correct. Accuracy: 98.413\n",
      "Epoch: 44\n",
      "Training dataset: 34754 out of 35700 correct. Accuracy: 97.35. Loss: 0.084\n",
      "Testing dataset: 6214 out of 6300 correct. Accuracy: 98.635\n",
      "Epoch: 45\n",
      "Training dataset: 34807 out of 35700 correct. Accuracy: 97.499. Loss: 0.077\n",
      "Testing dataset: 6162 out of 6300 correct. Accuracy: 97.81\n",
      "Epoch: 46\n",
      "Training dataset: 35024 out of 35700 correct. Accuracy: 98.106. Loss: 0.056\n",
      "Testing dataset: 5668 out of 6300 correct. Accuracy: 89.968\n",
      "Epoch: 47\n",
      "Training dataset: 35045 out of 35700 correct. Accuracy: 98.165. Loss: 0.052\n",
      "Testing dataset: 6218 out of 6300 correct. Accuracy: 98.698\n",
      "Epoch: 48\n",
      "Training dataset: 34756 out of 35700 correct. Accuracy: 97.356. Loss: 0.086\n",
      "Testing dataset: 6196 out of 6300 correct. Accuracy: 98.349\n",
      "Epoch: 49\n",
      "Training dataset: 34908 out of 35700 correct. Accuracy: 97.782. Loss: 0.067\n",
      "Testing dataset: 6220 out of 6300 correct. Accuracy: 98.73\n",
      "Epoch: 50\n",
      "Training dataset: 35190 out of 35700 correct. Accuracy: 98.571. Loss: 0.037\n",
      "Testing dataset: 6217 out of 6300 correct. Accuracy: 98.683\n",
      "Epoch: 51\n",
      "Training dataset: 34334 out of 35700 correct. Accuracy: 96.174. Loss: 0.118\n",
      "Testing dataset: 6197 out of 6300 correct. Accuracy: 98.365\n",
      "Epoch: 52\n",
      "Training dataset: 34771 out of 35700 correct. Accuracy: 97.398. Loss: 0.078\n",
      "Testing dataset: 6188 out of 6300 correct. Accuracy: 98.222\n",
      "Epoch: 53\n",
      "Training dataset: 35137 out of 35700 correct. Accuracy: 98.423. Loss: 0.043\n",
      "Testing dataset: 6210 out of 6300 correct. Accuracy: 98.571\n",
      "Epoch: 54\n",
      "Training dataset: 34619 out of 35700 correct. Accuracy: 96.972. Loss: 0.093\n",
      "Testing dataset: 6199 out of 6300 correct. Accuracy: 98.397\n",
      "Epoch: 55\n",
      "Training dataset: 34908 out of 35700 correct. Accuracy: 97.782. Loss: 0.065\n",
      "Testing dataset: 5806 out of 6300 correct. Accuracy: 92.159\n",
      "Epoch: 56\n",
      "Training dataset: 34887 out of 35700 correct. Accuracy: 97.723. Loss: 0.072\n",
      "Testing dataset: 6064 out of 6300 correct. Accuracy: 96.254\n",
      "Epoch: 57\n",
      "Training dataset: 34943 out of 35700 correct. Accuracy: 97.88. Loss: 0.064\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing dataset: 6187 out of 6300 correct. Accuracy: 98.206\n",
      "Epoch: 58\n",
      "Training dataset: 35119 out of 35700 correct. Accuracy: 98.373. Loss: 0.046\n",
      "Testing dataset: 5164 out of 6300 correct. Accuracy: 81.968\n",
      "Epoch: 59\n",
      "Training dataset: 34613 out of 35700 correct. Accuracy: 96.955. Loss: 0.094\n",
      "Testing dataset: 6212 out of 6300 correct. Accuracy: 98.603\n",
      "Epoch: 60\n",
      "Training dataset: 35200 out of 35700 correct. Accuracy: 98.599. Loss: 0.036\n",
      "Testing dataset: 6188 out of 6300 correct. Accuracy: 98.222\n",
      "Epoch: 61\n",
      "Training dataset: 34426 out of 35700 correct. Accuracy: 96.431. Loss: 0.113\n",
      "Testing dataset: 6218 out of 6300 correct. Accuracy: 98.698\n",
      "Epoch: 62\n",
      "Training dataset: 34826 out of 35700 correct. Accuracy: 97.552. Loss: 0.077\n",
      "Testing dataset: 6101 out of 6300 correct. Accuracy: 96.841\n",
      "Epoch: 63\n",
      "Training dataset: 35114 out of 35700 correct. Accuracy: 98.359. Loss: 0.046\n",
      "Testing dataset: 6216 out of 6300 correct. Accuracy: 98.667\n",
      "Epoch: 64\n",
      "Training dataset: 34769 out of 35700 correct. Accuracy: 97.392. Loss: 0.081\n",
      "Testing dataset: 6201 out of 6300 correct. Accuracy: 98.429\n",
      "Epoch: 65\n",
      "Training dataset: 35183 out of 35700 correct. Accuracy: 98.552. Loss: 0.036\n",
      "Testing dataset: 6208 out of 6300 correct. Accuracy: 98.54\n",
      "Epoch: 66\n",
      "Training dataset: 34295 out of 35700 correct. Accuracy: 96.064. Loss: 0.128\n",
      "Testing dataset: 6198 out of 6300 correct. Accuracy: 98.381\n",
      "Epoch: 67\n",
      "Training dataset: 34843 out of 35700 correct. Accuracy: 97.599. Loss: 0.07\n",
      "Testing dataset: 6204 out of 6300 correct. Accuracy: 98.476\n",
      "Epoch: 68\n",
      "Training dataset: 34837 out of 35700 correct. Accuracy: 97.583. Loss: 0.072\n",
      "Testing dataset: 5850 out of 6300 correct. Accuracy: 92.857\n",
      "Epoch: 69\n",
      "Training dataset: 34964 out of 35700 correct. Accuracy: 97.938. Loss: 0.063\n",
      "Testing dataset: 6162 out of 6300 correct. Accuracy: 97.81\n",
      "Epoch: 70\n",
      "Training dataset: 34942 out of 35700 correct. Accuracy: 97.877. Loss: 0.067\n",
      "Testing dataset: 6155 out of 6300 correct. Accuracy: 97.698\n",
      "Epoch: 71\n",
      "Training dataset: 35159 out of 35700 correct. Accuracy: 98.485. Loss: 0.039\n",
      "Testing dataset: 6217 out of 6300 correct. Accuracy: 98.683\n",
      "Epoch: 72\n",
      "Training dataset: 34566 out of 35700 correct. Accuracy: 96.824. Loss: 0.103\n",
      "Testing dataset: 6205 out of 6300 correct. Accuracy: 98.492\n",
      "Epoch: 73\n",
      "Training dataset: 35054 out of 35700 correct. Accuracy: 98.19. Loss: 0.049\n",
      "Testing dataset: 4156 out of 6300 correct. Accuracy: 65.968\n",
      "Epoch: 74\n",
      "Training dataset: 34749 out of 35700 correct. Accuracy: 97.336. Loss: 0.082\n",
      "Testing dataset: 6136 out of 6300 correct. Accuracy: 97.397\n",
      "Epoch: 75\n",
      "Training dataset: 34844 out of 35700 correct. Accuracy: 97.602. Loss: 0.076\n",
      "Testing dataset: 6199 out of 6300 correct. Accuracy: 98.397\n",
      "Epoch: 76\n",
      "Training dataset: 35098 out of 35700 correct. Accuracy: 98.314. Loss: 0.048\n",
      "Testing dataset: 6167 out of 6300 correct. Accuracy: 97.889\n",
      "Epoch: 77\n",
      "Training dataset: 34879 out of 35700 correct. Accuracy: 97.7. Loss: 0.072\n",
      "Testing dataset: 6216 out of 6300 correct. Accuracy: 98.667\n",
      "Epoch: 78\n",
      "Training dataset: 35009 out of 35700 correct. Accuracy: 98.064. Loss: 0.054\n",
      "Testing dataset: 6218 out of 6300 correct. Accuracy: 98.698\n",
      "Epoch: 79\n",
      "Training dataset: 34595 out of 35700 correct. Accuracy: 96.905. Loss: 0.09\n",
      "Testing dataset: 5792 out of 6300 correct. Accuracy: 91.937\n",
      "Epoch: 80\n",
      "Training dataset: 34926 out of 35700 correct. Accuracy: 97.832. Loss: 0.068\n",
      "Testing dataset: 6215 out of 6300 correct. Accuracy: 98.651\n",
      "Epoch: 81\n",
      "Training dataset: 34749 out of 35700 correct. Accuracy: 97.336. Loss: 0.073\n",
      "Testing dataset: 4812 out of 6300 correct. Accuracy: 76.381\n",
      "Epoch: 82\n",
      "Training dataset: 34671 out of 35700 correct. Accuracy: 97.118. Loss: 0.091\n",
      "Testing dataset: 5885 out of 6300 correct. Accuracy: 93.413\n",
      "Epoch: 83\n",
      "Training dataset: 34948 out of 35700 correct. Accuracy: 97.894. Loss: 0.061\n",
      "Testing dataset: 6217 out of 6300 correct. Accuracy: 98.683\n",
      "Epoch: 84\n",
      "Training dataset: 34542 out of 35700 correct. Accuracy: 96.756. Loss: 0.096\n",
      "Testing dataset: 5646 out of 6300 correct. Accuracy: 89.619\n",
      "Epoch: 85\n",
      "Training dataset: 34903 out of 35700 correct. Accuracy: 97.768. Loss: 0.067\n",
      "Testing dataset: 6212 out of 6300 correct. Accuracy: 98.603\n",
      "Epoch: 86\n",
      "Training dataset: 34925 out of 35700 correct. Accuracy: 97.829. Loss: 0.068\n",
      "Testing dataset: 5810 out of 6300 correct. Accuracy: 92.222\n",
      "Epoch: 87\n",
      "Training dataset: 34860 out of 35700 correct. Accuracy: 97.647. Loss: 0.073\n",
      "Testing dataset: 6198 out of 6300 correct. Accuracy: 98.381\n",
      "Epoch: 88\n",
      "Training dataset: 34903 out of 35700 correct. Accuracy: 97.768. Loss: 0.066\n",
      "Testing dataset: 6172 out of 6300 correct. Accuracy: 97.968\n",
      "Epoch: 89\n",
      "Training dataset: 34877 out of 35700 correct. Accuracy: 97.695. Loss: 0.07\n",
      "Testing dataset: 5432 out of 6300 correct. Accuracy: 86.222\n",
      "Epoch: 90\n",
      "Training dataset: 35029 out of 35700 correct. Accuracy: 98.12. Loss: 0.056\n",
      "Testing dataset: 6214 out of 6300 correct. Accuracy: 98.635\n",
      "Epoch: 91\n",
      "Training dataset: 34753 out of 35700 correct. Accuracy: 97.347. Loss: 0.084\n",
      "Testing dataset: 6184 out of 6300 correct. Accuracy: 98.159\n",
      "Epoch: 92\n",
      "Training dataset: 34882 out of 35700 correct. Accuracy: 97.709. Loss: 0.07\n",
      "Testing dataset: 5747 out of 6300 correct. Accuracy: 91.222\n",
      "Epoch: 93\n",
      "Training dataset: 35028 out of 35700 correct. Accuracy: 98.118. Loss: 0.055\n",
      "Testing dataset: 6052 out of 6300 correct. Accuracy: 96.063\n",
      "Epoch: 94\n",
      "Training dataset: 34940 out of 35700 correct. Accuracy: 97.871. Loss: 0.066\n",
      "Testing dataset: 6080 out of 6300 correct. Accuracy: 96.508\n",
      "Epoch: 95\n",
      "Training dataset: 34828 out of 35700 correct. Accuracy: 97.557. Loss: 0.076\n",
      "Testing dataset: 6026 out of 6300 correct. Accuracy: 95.651\n",
      "Epoch: 96\n",
      "Training dataset: 35037 out of 35700 correct. Accuracy: 98.143. Loss: 0.056\n",
      "Testing dataset: 6221 out of 6300 correct. Accuracy: 98.746\n",
      "Epoch: 97\n",
      "Training dataset: 34855 out of 35700 correct. Accuracy: 97.633. Loss: 0.074\n",
      "Testing dataset: 6035 out of 6300 correct. Accuracy: 95.794\n",
      "Epoch: 98\n",
      "Training dataset: 34914 out of 35700 correct. Accuracy: 97.798. Loss: 0.066\n",
      "Testing dataset: 6033 out of 6300 correct. Accuracy: 95.762\n",
      "Epoch: 99\n",
      "Training dataset: 35011 out of 35700 correct. Accuracy: 98.07. Loss: 0.055\n",
      "Testing dataset: 6213 out of 6300 correct. Accuracy: 98.619\n",
      "Epoch: 100\n",
      "Training dataset: 35196 out of 35700 correct. Accuracy: 98.588. Loss: 0.036\n",
      "Testing dataset: 6197 out of 6300 correct. Accuracy: 98.365\n",
      "Finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace=True)\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (fc): Linear(in_features=512, out_features=7, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_nn(resnet18, train_loader, test_loader, loss_func, optimizer, n_epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d8da393c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Epoch: 96, Best Accuracy: 98.746\n"
     ]
    }
   ],
   "source": [
    "checkpoint = torch.load(f'{CHECKPOINT_PATH}\\model_best_checkpoint.pth.tar')\n",
    "print(f\"Best Epoch: {checkpoint['epoch']}, Best Accuracy: {round(checkpoint['best accuracy'], 3)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6181b0af",
   "metadata": {},
   "outputs": [],
   "source": [
    "ONNX_SAVE_PATH = r\"C:\\Users\\offco\\Documents\\Dev_Projects\\BookCoverClassifier\\Test_Run\\Quantized Model\\pytorch_resnet18.onnx\"\n",
    "\n",
    "resnet18 = models.resnet18(pretrained=False)\n",
    "num_features = resnet18.fc.in_features\n",
    "num_classes = 7\n",
    "resnet18.fc = nn.Linear(num_features, num_classes)\n",
    "resnet18.load_state_dict(checkpoint['model'])\n",
    "\n",
    "dummy_input = torch.randn(1, 3, 256, 256, dtype=torch.float32)\n",
    "torch.onnx.export(\n",
    "    resnet18, \n",
    "    dummy_input, \n",
    "    ONNX_SAVE_PATH,\n",
    "    verbose=False,\n",
    "    export_params=True,\n",
    "    opset_version=11,\n",
    "    input_names = ['image'],\n",
    "    output_names = ['pred']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89a995eb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
